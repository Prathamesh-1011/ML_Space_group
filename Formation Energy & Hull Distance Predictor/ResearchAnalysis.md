# ğŸ”¬ Comparative Analysis: Formation Energy & Hull Distance Prediction Models


# 1ï¸âƒ£ **CrabNet**

## 1.1 Objective (Stated Explicitly)

* Predict **materials properties using composition only**
* Formation energy is the **primary benchmark task**

ğŸ“ *Page 2, Introduction*

## 1.2 Dataset

### Source

* **Materials Project (MP)**
* ~**132,000 inorganic compounds**

ğŸ“ *Page 3, â€œDatasetsâ€ section*

### Target Variable

* **Formation energy per atom (eV/atom)**
* DFT-computed, PBE functional


## 1.3 Data Preparation & Preprocessing

### Composition Parsing

* Chemical formula parsed into:

  * List of elements
  * Corresponding stoichiometric fractions

ğŸ“ *Page 4, Figure 1 caption*

### Encoding

* **No handcrafted features**
* Each element represented by:

  * Learned embedding vector
  * Explicit **fractional amount**

> â— No Magpie, no matminer, no physical descriptors

ğŸ“ *Page 4*

### Normalization

* Target normalized to zero mean, unit variance during training

ğŸ“ *Page 5, Training details*


## 1.4 Model Architecture

### Core Model

* **Transformer-based self-attention network**

ğŸ“ *Page 4, Figure 1*

### Architecture Components

1. Element embedding layer
2. Fractional embedding concatenation
3. Multi-head self-attention blocks
4. Feed-forward projection
5. Mean pooling
6. Regression head

ğŸ“ *Page 4â€“5*

### Key Innovation

* **Fractional encoding** â†’ attention weighted by stoichiometry


## 1.5 Training Methodology

* Optimizer: **Adam**
* Loss: **Mean Absolute Error (MAE)**
* Learning rate scheduling: yes
* Batch size: specified in appendix

ğŸ“ *Page 5*


## 1.6 Validation Strategy

* Random **train/validation/test split**
* No compositional holdout
* No leave-one-element-out validation

ğŸ“ *Page 6*


## 1.7 Results (Exact Location)

### Formation Energy (MP)

* **MAE = 0.0296 eV/atom**

ğŸ“ **Page 6, Table 1**


## 1.8 Limitations (Explicitly Stated)

* No uncertainty quantification
* No structural sensitivity
* Limited extrapolation to unseen chemistries

ğŸ“ *Page 9, Discussion*


# 2ï¸âƒ£ **Roost**

## 2.1 Objective

* Predict formation energy **without crystal structure**
* Learn chemically meaningful element embeddings

ğŸ“ *Page 2*

## 2.2 Dataset

### Source

* **OQMD** (~256k compounds)
* **Materials Project** used for transfer

ğŸ“ *Page 3*

### Target

* Formation energy per atom


## 2.3 Data Preparation

### Composition Graph Construction

* Nodes = elements
* Fully connected graph
* Edge weights = product of element fractions

ğŸ“ *Page 3, Figure 1*

### Element Initialization

* Pretrained **MatScholar embeddings**

ğŸ“ *Page 3*


## 2.4 Model Architecture

### Model Type

* **Message Passing Neural Network (MPNN)**

### Architecture

1. Element embedding
2. Fraction-weighted message passing
3. Attention aggregation
4. Global pooling
5. Regression head

ğŸ“ *Page 3â€“4*


## 2.5 Training Methodology

* Optimizer: Adam
* Loss: MAE
* Ensemble of **10 independent models**

ğŸ“ *Page 4*


## 2.6 Validation

* Hold-out test set
* Learning curves vs dataset size
* Uncertainty calibration curves

ğŸ“ *Page 4â€“5*


## 2.7 Results

### Formation Energy

* Single model: **MAE = 0.0297 eV**
* Ensemble: **MAE = 0.0241 eV**

ğŸ“ **Page 4, Table 1**


## 2.8 Limitations

* No structure â†’ polymorphs indistinguishable
* Higher inference cost than CrabNet

ğŸ“ *Page 6*


# 3ï¸âƒ£ **CGCNN**


## 3.1 Objective

* Predict formation energy **from crystal structure**

ğŸ“ *Page 1*


## 3.2 Dataset

* Materials Project
* ~47,000 relaxed structures

ğŸ“ *Page 4*


## 3.3 Data Processing

### Graph Construction

* Nodes = atoms
* Edges = neighbors within cutoff
* Edge features = Gaussian-expanded distances

ğŸ“ *Page 2*


## 3.4 Architecture

* Crystal graph convolution layers
* Pooling â†’ dense regression head

ğŸ“ *Page 2â€“3*


## 3.5 Validation

* Random split
* MAE metric

ğŸ“ *Page 5*


## 3.6 Results

* Formation Energy MAE = **0.039 eV/atom**

ğŸ“ **Page 5, Table 2**


## 3.7 Limitations

* Requires structure
* Lower accuracy than newer GNNs

ğŸ“ *Page 6*


# 4ï¸âƒ£ **MEGNet**


## 4.1 Objective

* Universal graph network for molecules & crystals

ğŸ“ *Page 1*


## 4.2 Dataset

* Materials Project (~69k)

ğŸ“ *Page 2*


## 4.3 Data Processing

* Nodes: atoms
* Edges: bonds
* Global state vector (optional)

ğŸ“ *Page 2*


## 4.4 Architecture

* Graph network with:

  * Node update
  * Edge update
  * State update
* Set2Set pooling

ğŸ“ *Page 2â€“3*


## 4.5 Validation

* Transfer learning evaluation
* Cross-property tests

ğŸ“ *Page 4*


## 4.6 Results

* Formation Energy MAE = **0.028 eV/atom**

ğŸ“ **Page 3, Table 1**


## 4.7 Limitations

* Needs structure
* Computationally heavier

ğŸ“ *Page 5*


# 5ï¸âƒ£ **DeeperGATGNN**

## 5.1 Objective

* Overcome GNN oversmoothing
* Improve formation energy prediction

ğŸ“ *Page 1*

## 5.2 Dataset

* MatBench datasets
* MP-derived formation energy sets

ğŸ“ *Page 7*


## 5.3 Architecture

* Graph Attention Network
* 20â€“30 layers
* Residual connections + group normalization

ğŸ“ *Page 6*


## 5.4 Validation

* MatBench benchmark protocol

ğŸ“ *Page 8*


## 5.5 Results

* Formation Energy MAE â‰ˆ **0.032 eV/atom**

ğŸ“ **Page 8, Table 3**


## 5.6 Limitations

* Requires large data
* Not suitable for early screening

ğŸ“ *Page 10*


# ğŸ¯ FINAL COMPARATIVE CONCLUSION (FACT-BASED)

| Model            | Structure Needed | FE MAE (eV) | Page |
| ---------------- | ---------------- | ----------- | ---- |
| CrabNet          | âŒ                | **0.0296**  | p6   |
| Roost (ensemble) | âŒ                | **0.0241**  | p4   |
| CGCNN            | âœ…                | 0.039       | p5   |
| MEGNet           | âœ…                | 0.028       | p3   |
| DeeperGATGNN     | âœ…                | 0.032       | p8   |
